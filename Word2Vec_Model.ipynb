{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "LSTM, BiLSTM, RNN with Word2Vec:"
      ],
      "metadata": {
        "id": "MRb9qo2dyy00"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzNYbcFNAYVg",
        "outputId": "79628753-f97d-4f3e-d9c7-2156ffc54f51"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns in the dataset: Index(['Airport Service Freeform Feedback'], dtype='object')\n",
            "Using column 'Airport Service Freeform Feedback' for processing.\n",
            "Training RNN Model...\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1678/1678\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 41ms/step - accuracy: 0.4971 - loss: 0.6932 - val_accuracy: 0.4996 - val_loss: 0.6933\n",
            "Epoch 2/5\n",
            "\u001b[1m1678/1678\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 38ms/step - accuracy: 0.4987 - loss: 0.6932 - val_accuracy: 0.4996 - val_loss: 0.6932\n",
            "Epoch 3/5\n",
            "\u001b[1m1678/1678\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 39ms/step - accuracy: 0.5018 - loss: 0.6932 - val_accuracy: 0.5004 - val_loss: 0.6932\n",
            "Epoch 4/5\n",
            "\u001b[1m1678/1678\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 38ms/step - accuracy: 0.4988 - loss: 0.6932 - val_accuracy: 0.4996 - val_loss: 0.6932\n",
            "Epoch 5/5\n",
            "\u001b[1m1678/1678\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 39ms/step - accuracy: 0.5001 - loss: 0.6931 - val_accuracy: 0.4996 - val_loss: 0.6932\n",
            "\u001b[1m420/420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.5035 - loss: 0.6931\n",
            "RNN Model Test Accuracy: 49.96%\n",
            "Training LSTM Model...\n",
            "Epoch 1/5\n",
            "\u001b[1m1678/1678\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m271s\u001b[0m 160ms/step - accuracy: 0.5007 - loss: 0.6932 - val_accuracy: 0.4996 - val_loss: 0.6932\n",
            "Epoch 2/5\n",
            "\u001b[1m1678/1678\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 157ms/step - accuracy: 0.5012 - loss: 0.6932 - val_accuracy: 0.5004 - val_loss: 0.6931\n",
            "Epoch 3/5\n",
            "\u001b[1m1678/1678\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 157ms/step - accuracy: 0.4991 - loss: 0.6932 - val_accuracy: 0.4996 - val_loss: 0.6932\n",
            "Epoch 4/5\n",
            "\u001b[1m1678/1678\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m351s\u001b[0m 174ms/step - accuracy: 0.5020 - loss: 0.6932 - val_accuracy: 0.5004 - val_loss: 0.6932\n",
            "Epoch 5/5\n",
            "\u001b[1m1678/1678\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m270s\u001b[0m 161ms/step - accuracy: 0.4967 - loss: 0.6932 - val_accuracy: 0.5004 - val_loss: 0.6932\n",
            "\u001b[1m420/420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 49ms/step - accuracy: 0.4965 - loss: 0.6932\n",
            "LSTM Model Test Accuracy: 50.04%\n",
            "Training BiLSTM Model...\n",
            "Epoch 1/5\n",
            "\u001b[1m1678/1678\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m506s\u001b[0m 299ms/step - accuracy: 0.4985 - loss: 0.6932 - val_accuracy: 0.5004 - val_loss: 0.6933\n",
            "Epoch 2/5\n",
            "\u001b[1m1678/1678\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m500s\u001b[0m 298ms/step - accuracy: 0.4966 - loss: 0.6933 - val_accuracy: 0.4996 - val_loss: 0.6931\n",
            "Epoch 3/5\n",
            "\u001b[1m1678/1678\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m488s\u001b[0m 291ms/step - accuracy: 0.4967 - loss: 0.6932 - val_accuracy: 0.5004 - val_loss: 0.6931\n",
            "Epoch 4/5\n",
            "\u001b[1m1678/1678\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m516s\u001b[0m 299ms/step - accuracy: 0.5025 - loss: 0.6932 - val_accuracy: 0.4988 - val_loss: 0.6932\n",
            "Epoch 5/5\n",
            "\u001b[1m1678/1678\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m497s\u001b[0m 296ms/step - accuracy: 0.5004 - loss: 0.6932 - val_accuracy: 0.4996 - val_loss: 0.6932\n",
            "\u001b[1m420/420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 91ms/step - accuracy: 0.5035 - loss: 0.6931\n",
            "BiLSTM Model Test Accuracy: 49.96%\n",
            "\n",
            "Total Accuracy of Each Model:\n",
            "RNN Accuracy: 49.96%\n",
            "LSTM Accuracy: 50.04%\n",
            "BiLSTM Accuracy: 49.96%\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM, SimpleRNN, Bidirectional\n",
        "from tensorflow.keras import backend as K\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Google Colab Setup: Uncomment if running on Colab\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# data_path = '/content/drive/MyDrive/Airport feedback.xlsx'\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_excel('Airport feedback.xlsx')  # Change path to `data_path` for Colab setup\n",
        "\n",
        "# Inspect the column names\n",
        "print(\"Columns in the dataset:\", data.columns)\n",
        "\n",
        "# Use the correct column name (ensure no extra spaces)\n",
        "data.rename(columns=lambda x: x.strip(), inplace=True)\n",
        "\n",
        "# Replace 'Feedback' with the actual column name from your dataset\n",
        "feedback_column = 'Airport Service Freeform Feedback'  # Replace with the correct column if different\n",
        "print(f\"Using column '{feedback_column}' for processing.\")\n",
        "\n",
        "# Preprocessing the dataset\n",
        "def preprocess_text(text):\n",
        "    if pd.isna(text):  # Handle NaN values\n",
        "        return \"\"\n",
        "    text = str(text).lower()  # Convert to lowercase\n",
        "    text = re.sub(r'[^a-zA-Z\\\\s]', '', text)  # Remove special characters and numbers\n",
        "    return text\n",
        "\n",
        "# Clean and preprocess data\n",
        "data[feedback_column] = data[feedback_column].apply(preprocess_text)\n",
        "\n",
        "# Generate random binary labels (Replace with actual labels if available)\n",
        "y = np.random.randint(2, size=len(data[feedback_column]))\n",
        "\n",
        "# Split data into features (X) and labels (y)\n",
        "X = data[feedback_column]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert X_train and X_test to lists of strings\n",
        "X_train = X_train.tolist()\n",
        "X_test = X_test.tolist()\n",
        "\n",
        "# Tokenization and Padding\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "maxlen = 100\n",
        "X_train_pad = pad_sequences(X_train_seq, padding='post', maxlen=maxlen)\n",
        "X_test_pad = pad_sequences(X_test_seq, padding='post', maxlen=maxlen)\n",
        "\n",
        "# Word2Vec Embedding\n",
        "word2vec_model = Word2Vec(sentences=[text.split() for text in X_train if text], vector_size=100, window=5, min_count=1, workers=4)\n",
        "embedding_matrix = np.zeros((vocab_size, 100))\n",
        "\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if word in word2vec_model.wv:\n",
        "        embedding_matrix[i] = word2vec_model.wv[word]\n",
        "\n",
        "# Define RNN Model\n",
        "def create_rnn_model():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=vocab_size, output_dim=100, weights=[embedding_matrix], input_length=maxlen, trainable=False))\n",
        "    model.add(SimpleRNN(128, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Define LSTM Model\n",
        "def create_lstm_model():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=vocab_size, output_dim=100, weights=[embedding_matrix], input_length=maxlen, trainable=False))\n",
        "    model.add(LSTM(128, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Define BiLSTM Model\n",
        "def create_bilstm_model():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=vocab_size, output_dim=100, weights=[embedding_matrix], input_length=maxlen, trainable=False))\n",
        "    model.add(Bidirectional(LSTM(128, activation='relu')))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Train and Evaluate Models\n",
        "\n",
        "# Train RNN Model\n",
        "rnn_model = create_rnn_model()\n",
        "print(\"Training RNN Model...\")\n",
        "rnn_model.fit(X_train_pad, y_train, epochs=5, batch_size=32, validation_data=(X_test_pad, y_test))\n",
        "\n",
        "# Calculate RNN accuracy\n",
        "rnn_accuracy = rnn_model.evaluate(X_test_pad, y_test)[1]\n",
        "print(f\"RNN Model Test Accuracy: {rnn_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Train LSTM Model\n",
        "lstm_model = create_lstm_model()\n",
        "print(\"Training LSTM Model...\")\n",
        "lstm_model.fit(X_train_pad, y_train, epochs=5, batch_size=32, validation_data=(X_test_pad, y_test))\n",
        "\n",
        "# Calculate LSTM accuracy\n",
        "lstm_accuracy = lstm_model.evaluate(X_test_pad, y_test)[1]\n",
        "print(f\"LSTM Model Test Accuracy: {lstm_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Train BiLSTM Model\n",
        "bilstm_model = create_bilstm_model()\n",
        "print(\"Training BiLSTM Model...\")\n",
        "bilstm_model.fit(X_train_pad, y_train, epochs=5, batch_size=32, validation_data=(X_test_pad, y_test))\n",
        "\n",
        "# Calculate BiLSTM accuracy\n",
        "bilstm_accuracy = bilstm_model.evaluate(X_test_pad, y_test)[1]\n",
        "print(f\"BiLSTM Model Test Accuracy: {bilstm_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Display final total accuracy for each model\n",
        "print(\"\\nTotal Accuracy of Each Model:\")\n",
        "print(f\"RNN Accuracy: {rnn_accuracy * 100:.2f}%\")\n",
        "print(f\"LSTM Accuracy: {lstm_accuracy * 100:.2f}%\")\n",
        "print(f\"BiLSTM Accuracy: {bilstm_accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM, BiLSTM, RNN with Glove:"
      ],
      "metadata": {
        "id": "umcKDGaZzFbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM, SimpleRNN, Bidirectional\n",
        "from tensorflow.keras import backend as K\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Google Colab Setup: Uncomment if running on Colab\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# data_path = '/content/drive/MyDrive/Airport feedback.xlsx'\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_excel('Airport feedback.xlsx')  # Change path to `data_path` for Colab setup\n",
        "\n",
        "# Inspect the column names\n",
        "print(\"Columns in the dataset:\", data.columns)\n",
        "\n",
        "# Use the correct column name (ensure no extra spaces)\n",
        "data.rename(columns=lambda x: x.strip(), inplace=True)\n",
        "\n",
        "# Replace 'Feedback' with the actual column name from your dataset\n",
        "feedback_column = 'Airport Service Freeform Feedback'  # Replace with the correct column if different\n",
        "print(f\"Using column '{feedback_column}' for processing.\")\n",
        "\n",
        "# Preprocessing the dataset\n",
        "def preprocess_text(text):\n",
        "    if pd.isna(text):  # Handle NaN values\n",
        "        return \"\"\n",
        "    text = str(text).lower()  # Convert to lowercase\n",
        "    text = re.sub(r'[^a-zA-Z\\\\s]', '', text)  # Remove special characters and numbers\n",
        "    return text\n",
        "\n",
        "# Clean and preprocess data\n",
        "data[feedback_column] = data[feedback_column].apply(preprocess_text)\n",
        "\n",
        "# Generate random binary labels (Replace with actual labels if available)\n",
        "y = np.random.randint(2, size=len(data[feedback_column]))\n",
        "\n",
        "# Split data into features (X) and labels (y)\n",
        "X = data[feedback_column]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert X_train and X_test to lists of strings\n",
        "X_train = X_train.tolist()\n",
        "X_test = X_test.tolist()\n",
        "\n",
        "# Tokenization and Padding\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "maxlen = 100\n",
        "X_train_pad = pad_sequences(X_train_seq, padding='post', maxlen=maxlen)\n",
        "X_test_pad = pad_sequences(X_test_seq, padding='post', maxlen=maxlen)\n",
        "\n",
        "# GloVe Embedding (load GloVe vectors from file)\n",
        "def load_glove_embeddings(glove_file_path, tokenizer):\n",
        "    embeddings_index = {}\n",
        "    with open(glove_file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "    print(f'Found {len(embeddings_index)} word vectors.')\n",
        "\n",
        "    # Create the embedding matrix\n",
        "    embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, 100))\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        if word in embeddings_index:\n",
        "            embedding_matrix[i] = embeddings_index[word]\n",
        "    return embedding_matrix\n",
        "\n",
        "# Load the GloVe word vectors (ensure to provide the correct path to the GloVe file)\n",
        "glove_file_path = '/content/glove.6B.100d.txt'  # Provide path to GloVe file\n",
        "embedding_matrix = load_glove_embeddings(glove_file_path, tokenizer)\n",
        "\n",
        "# Define RNN Model\n",
        "def create_rnn_model():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=vocab_size, output_dim=100, weights=[embedding_matrix], input_length=maxlen, trainable=False))\n",
        "    model.add(SimpleRNN(128, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Define LSTM Model\n",
        "def create_lstm_model():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=vocab_size, output_dim=100, weights=[embedding_matrix], input_length=maxlen, trainable=False))\n",
        "    model.add(LSTM(128, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Define BiLSTM Model\n",
        "def create_bilstm_model():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=vocab_size, output_dim=100, weights=[embedding_matrix], input_length=maxlen, trainable=False))\n",
        "    model.add(Bidirectional(LSTM(128, activation='relu')))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Train and Evaluate Models\n",
        "\n",
        "# Train RNN Model\n",
        "rnn_model = create_rnn_model()\n",
        "print(\"Training RNN Model...\")\n",
        "rnn_model.fit(X_train_pad, y_train, epochs=5, batch_size=32, validation_data=(X_test_pad, y_test))\n",
        "\n",
        "# Calculate RNN accuracy\n",
        "rnn_accuracy = rnn_model.evaluate(X_test_pad, y_test)[1]\n",
        "print(f\"RNN Model Test Accuracy: {rnn_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Train LSTM Model\n",
        "lstm_model = create_lstm_model()\n",
        "print(\"Training LSTM Model...\")\n",
        "lstm_model.fit(X_train_pad, y_train, epochs=5, batch_size=32, validation_data=(X_test_pad, y_test))\n",
        "\n",
        "# Calculate LSTM accuracy\n",
        "lstm_accuracy = lstm_model.evaluate(X_test_pad, y_test)[1]\n",
        "print(f\"LSTM Model Test Accuracy: {lstm_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Train BiLSTM Model\n",
        "bilstm_model = create_bilstm_model()\n",
        "print(\"Training BiLSTM Model...\")\n",
        "bilstm_model.fit(X_train_pad, y_train, epochs=5, batch_size=32, validation_data=(X_test_pad, y_test))\n",
        "\n",
        "# Calculate BiLSTM accuracy\n",
        "bilstm_accuracy = bilstm_model.evaluate(X_test_pad, y_test)[1]\n",
        "print(f\"BiLSTM Model Test Accuracy: {bilstm_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Display final total accuracy for each model\n",
        "print(\"\\nTotal Accuracy of Each Model:\")\n",
        "print(f\"RNN Accuracy: {rnn_accuracy * 100:.2f}%\")\n",
        "print(f\"LSTM Accuracy: {lstm_accuracy * 100:.2f}%\")\n",
        "print(f\"BiLSTM Accuracy: {bilstm_accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFmnKsBbzZRh",
        "outputId": "83bd121e-c0af-4116-e6f6-8dd16ad61e80"
      },
      "execution_count": 5,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns in the dataset: Index(['Airport Service Freeform Feedback'], dtype='object')\n",
            "Using column 'Airport Service Freeform Feedback' for processing.\n",
            "Found 30555 word vectors.\n",
            "Training RNN Model...\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1678/1678\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 45ms/step - accuracy: 0.5031 - loss: 0.6932 - val_accuracy: 0.4980 - val_loss: 0.6932\n",
            "Epoch 2/5\n",
            "\u001b[1m1678/1678\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 45ms/step - accuracy: 0.4975 - loss: 0.6932 - val_accuracy: 0.5020 - val_loss: 0.6931\n",
            "Epoch 3/5\n",
            "\u001b[1m1678/1678\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 44ms/step - accuracy: 0.5014 - loss: 0.6932 - val_accuracy: 0.4980 - val_loss: 0.6932\n",
            "Epoch 4/5\n",
            "\u001b[1m1678/1678\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 41ms/step - accuracy: 0.5000 - loss: 0.6932 - val_accuracy: 0.4980 - val_loss: 0.6932\n",
            "Epoch 5/5\n",
            "\u001b[1m1678/1678\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 45ms/step - accuracy: 0.4987 - loss: 0.6932 - val_accuracy: 0.4980 - val_loss: 0.6932\n",
            "\u001b[1m420/420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 17ms/step - accuracy: 0.5012 - loss: 0.6931\n",
            "RNN Model Test Accuracy: 49.80%\n",
            "Training LSTM Model...\n",
            "Epoch 1/5\n",
            "\u001b[1m1678/1678\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m292s\u001b[0m 173ms/step - accuracy: 0.4955 - loss: 0.6932 - val_accuracy: 0.5020 - val_loss: 0.6931\n",
            "Epoch 2/5\n",
            "\u001b[1m1678/1678\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m278s\u001b[0m 166ms/step - accuracy: 0.4968 - loss: 0.6932 - val_accuracy: 0.4980 - val_loss: 0.6932\n",
            "Epoch 3/5\n",
            "\u001b[1m1678/1678\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m343s\u001b[0m 178ms/step - accuracy: 0.5003 - loss: 0.6932 - val_accuracy: 0.4980 - val_loss: 0.6931\n",
            "Epoch 4/5\n",
            "\u001b[1m1678/1678\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 167ms/step - accuracy: 0.4980 - loss: 0.6932 - val_accuracy: 0.4980 - val_loss: 0.6932\n",
            "Epoch 5/5\n",
            "\u001b[1m1678/1678\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m283s\u001b[0m 169ms/step - accuracy: 0.4945 - loss: 0.6932 - val_accuracy: 0.5020 - val_loss: 0.6931\n",
            "\u001b[1m420/420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 63ms/step - accuracy: 0.4988 - loss: 0.6931\n",
            "LSTM Model Test Accuracy: 50.20%\n",
            "Training BiLSTM Model...\n",
            "Epoch 1/5\n",
            "\u001b[1m1678/1678\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m518s\u001b[0m 307ms/step - accuracy: 0.5000 - loss: 0.6933 - val_accuracy: 0.4981 - val_loss: 0.6932\n",
            "Epoch 2/5\n",
            "\u001b[1m1678/1678\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m561s\u001b[0m 306ms/step - accuracy: 0.4993 - loss: 0.6931 - val_accuracy: 0.4984 - val_loss: 0.6931\n",
            "Epoch 3/5\n",
            "\u001b[1m1678/1678\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m562s\u001b[0m 306ms/step - accuracy: 0.4992 - loss: 0.6931 - val_accuracy: 0.4982 - val_loss: 0.6932\n",
            "Epoch 4/5\n",
            "\u001b[1m1678/1678\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m569s\u001b[0m 310ms/step - accuracy: 0.4944 - loss: 0.6930 - val_accuracy: 0.5023 - val_loss: 0.6931\n",
            "Epoch 5/5\n",
            "\u001b[1m1678/1678\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m519s\u001b[0m 310ms/step - accuracy: 0.4945 - loss: 0.6930 - val_accuracy: 0.5023 - val_loss: 0.6933\n",
            "\u001b[1m420/420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 98ms/step - accuracy: 0.4991 - loss: 0.6932\n",
            "BiLSTM Model Test Accuracy: 50.23%\n",
            "\n",
            "Total Accuracy of Each Model:\n",
            "RNN Accuracy: 49.80%\n",
            "LSTM Accuracy: 50.20%\n",
            "BiLSTM Accuracy: 50.23%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM, BiLSTM, RNN with FastText:"
      ],
      "metadata": {
        "id": "fdGa-XLOKI5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zh3G51H-pE7O",
        "outputId": "3f5d3bd7-cbf9-4b8c-806d-33ae6a852e3d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (75.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.26.4)\n",
            "Using cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.3-cp310-cp310-linux_x86_64.whl size=4296228 sha256=f7a568bba023db7f16e801bbfb55fdb691682a6c3f15f53a9a97a95f1ee5a8d8\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/a2/00/81db54d3e6a8199b829d58e02cec2ddb20ce3e59fad8d3c92a\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.3 pybind11-2.13.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM, SimpleRNN, Bidirectional\n",
        "from tensorflow.keras import backend as K\n",
        "import fasttext  # Importing the FastText library\n",
        "\n",
        "# Google Colab Setup: Uncomment if running on Colab\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# data_path = '/content/drive/MyDrive/Airport feedback.xlsx'\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_excel('Airport feedback.xlsx')  # Change path to `data_path` for Colab setup\n",
        "\n",
        "# Inspect the column names\n",
        "print(\"Columns in the dataset:\", data.columns)\n",
        "\n",
        "# Use the correct column name (ensure no extra spaces)\n",
        "data.rename(columns=lambda x: x.strip(), inplace=True)\n",
        "\n",
        "# Replace 'Feedback' with the actual column name from your dataset\n",
        "feedback_column = 'Airport Service Freeform Feedback'  # Replace with the correct column if different\n",
        "print(f\"Using column '{feedback_column}' for processing.\")\n",
        "\n",
        "# Preprocessing the dataset\n",
        "def preprocess_text(text):\n",
        "    if pd.isna(text):  # Handle NaN values\n",
        "        return \"\"\n",
        "    text = str(text).lower()  # Convert to lowercase\n",
        "    text = re.sub(r'[^a-zA-Z\\\\s]', '', text)  # Remove special characters and numbers\n",
        "    return text\n",
        "\n",
        "# Clean and preprocess data\n",
        "data[feedback_column] = data[feedback_column].apply(preprocess_text)\n",
        "\n",
        "# Generate random binary labels (Replace with actual labels if available)\n",
        "y = np.random.randint(2, size=len(data[feedback_column]))\n",
        "\n",
        "# Split data into features (X) and labels (y)\n",
        "X = data[feedback_column]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert X_train and X_test to lists of strings\n",
        "X_train = X_train.tolist()\n",
        "X_test = X_test.tolist()\n",
        "\n",
        "# Tokenization and Padding\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "maxlen = 100\n",
        "X_train_pad = pad_sequences(X_train_seq, padding='post', maxlen=maxlen)\n",
        "X_test_pad = pad_sequences(X_test_seq, padding='post', maxlen=maxlen)\n",
        "\n",
        "# FastText Embedding (load FastText vectors from file)\n",
        "def load_fasttext_embeddings(fasttext_file_path, tokenizer):\n",
        "    # Loading pre-trained FastText model\n",
        "    model = fasttext.load_model(fasttext_file_path)  # FastText model\n",
        "    print(f'Loaded FastText model.')\n",
        "\n",
        "    # Create the embedding matrix\n",
        "    embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, 100))  # 100-dimensional FastText embeddings\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        # Get word vector from FastText model\n",
        "        embedding_matrix[i] = model.get_word_vector(word)\n",
        "    return embedding_matrix\n",
        "\n",
        "# Load the FastText word vectors (ensure to provide the correct path to the FastText model)\n",
        "fasttext_file_path = 'cc.en.300.bin'  # Provide path to FastText pre-trained model\n",
        "embedding_matrix = load_fasttext_embeddings(fasttext_file_path, tokenizer)\n",
        "\n",
        "# Define RNN Model\n",
        "def create_rnn_model():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=vocab_size, output_dim=100, weights=[embedding_matrix], input_length=maxlen, trainable=False))\n",
        "    model.add(SimpleRNN(128, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Define LSTM Model\n",
        "def create_lstm_model():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=vocab_size, output_dim=100, weights=[embedding_matrix], input_length=maxlen, trainable=False))\n",
        "    model.add(LSTM(128, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Define BiLSTM Model\n",
        "def create_bilstm_model():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=vocab_size, output_dim=100, weights=[embedding_matrix], input_length=maxlen, trainable=False))\n",
        "    model.add(Bidirectional(LSTM(128, activation='relu')))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Train and Evaluate Models\n",
        "\n",
        "# Train RNN Model\n",
        "rnn_model = create_rnn_model()\n",
        "print(\"Training RNN Model...\")\n",
        "rnn_model.fit(X_train_pad, y_train, epochs=5, batch_size=32, validation_data=(X_test_pad, y_test))\n",
        "\n",
        "# Calculate RNN accuracy\n",
        "rnn_accuracy = rnn_model.evaluate(X_test_pad, y_test)[1]\n",
        "print(f\"RNN Model Test Accuracy: {rnn_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Train LSTM Model\n",
        "lstm_model = create_lstm_model()\n",
        "print(\"Training LSTM Model...\")\n",
        "lstm_model.fit(X_train_pad, y_train, epochs=5, batch_size=32, validation_data=(X_test_pad, y_test))\n",
        "\n",
        "# Calculate LSTM accuracy\n",
        "lstm_accuracy = lstm_model.evaluate(X_test_pad, y_test)[1]\n",
        "print(f\"LSTM Model Test Accuracy: {lstm_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Train BiLSTM Model\n",
        "bilstm_model = create_bilstm_model()\n",
        "print(\"Training BiLSTM Model...\")\n",
        "bilstm_model.fit(X_train_pad, y_train, epochs=5, batch_size=32, validation_data=(X_test_pad, y_test))\n",
        "\n",
        "# Calculate BiLSTM accuracy\n",
        "bilstm_accuracy = bilstm_model.evaluate(X_test_pad, y_test)[1]\n",
        "print(f\"BiLSTM Model Test Accuracy: {bilstm_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Display final total accuracy for each model\n",
        "print(\"\\nTotal Accuracy of Each Model:\")\n",
        "print(f\"RNN Accuracy: {rnn_accuracy * 100:.2f}%\")\n",
        "print(f\"LSTM Accuracy: {lstm_accuracy * 100:.2f}%\")\n",
        "print(f\"BiLSTM Accuracy: {bilstm_accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "fUYBscGoKHhI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}